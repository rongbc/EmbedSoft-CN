### 15.3 执行直接 I/O

大多数 I/O 操作都是通过内核进行缓冲的。使用内核空间缓冲区可以在用户空间和实际设备之间实现一定程度的隔离；这种隔离可以简化编程，并且在许多情况下还能带来性能提升。然而，在某些情况下，直接对用户空间缓冲区进行 I/O 操作可能会带来好处。如果要传输的数据量很大，直接在用户空间和设备间传输数据，而不通过内核空间进行额外的复制，可以加快操作速度。

在 2.6 内核中，SCSI 磁带驱动程序就是直接 I/O 的一个应用示例。流式磁带可以在系统中传输大量数据，并且磁带传输通常是面向记录的，因此在内核中缓冲数据并没有太大好处。所以，在条件合适时（例如，用户空间缓冲区按页对齐），SCSI 磁带驱动程序会直接进行 I/O 操作，而不复制数据。

话虽如此，但要认识到直接 I/O 并不总是能带来预期的性能提升。设置直接 I/O 的开销（包括处理相关用户页面的缺页异常和锁定页面）可能很大，并且会失去缓冲 I/O 的优势。例如，使用直接 I/O 要求 `write` 系统调用必须同步执行；否则，应用程序将不知道何时可以重用其 I/O 缓冲区。在每次写入完成之前阻塞应用程序可能会降低操作速度，这就是为什么使用直接 I/O 的应用程序通常也会使用异步 I/O 操作的原因。

无论如何，实际的经验教训是，在字符驱动程序中实现直接 I/O 通常是不必要的，甚至可能会有负面影响。只有当你确定缓冲 I/O 的开销确实在拖慢操作速度时，才应该考虑实现直接 I/O。此外，块设备驱动程序和网络驱动程序根本不需要担心实现直接 I/O；在这两种情况下，当需要时，内核中的高层代码会设置并使用直接 I/O，驱动层代码甚至不需要知道正在执行直接 I/O。

在 2.6 内核中实现直接 I/O 的关键是 `get_user_pages` 函数，该函数在 `<linux/mm.h>` 中声明，原型如下：
```c
int get_user_pages(struct task_struct *tsk, 
                   struct mm_struct *mm, 
                   unsigned long start,
                   int len, 
                   int write, 
                   int force, 
                   struct page **pages, 
                   struct vm_area_struct **vmas);
```
这个函数有几个参数：
- **tsk**：指向执行 I/O 操作的任务的指针；其主要目的是告诉内核在设置缓冲区时产生的任何页面错误应该由哪个任务负责。这个参数几乎总是传递 `current`。
- **mm**：指向描述要映射的地址空间的内存管理结构的指针。`mm_struct` 结构将进程虚拟地址空间的所有部分（VMA）联系在一起。对于驱动程序来说，这个参数应该始终是 `current->mm`。
- **start** 和 **len**：`start` 是用户空间缓冲区的（按页对齐的）地址，`len` 是缓冲区的页数。
- **write** 和 **force**：如果 `write` 不为零，则这些页面将被映射为可写访问（当然，这意味着用户空间正在执行读操作）。`force` 标志告诉 `get_user_pages` 忽略给定页面的保护，以提供所需的访问权限；驱动程序应该始终将此参数设置为 0。
- **pages** 和 **vmas**：输出参数。成功完成后，`pages` 包含一个指向描述用户空间缓冲区的 `struct page` 结构的指针列表，`vmas` 包含指向相关 VMA 的指针。显然，这些参数应该指向能够容纳至少 `len` 个指针的数组。这两个参数都可以为 `NULL`，但至少需要 `struct page` 指针才能实际操作缓冲区。

`get_user_pages` 是一个底层的内存管理函数，其接口相当复杂。在调用该函数之前，还需要以读模式获取地址空间的 `mmap` 读写信号量。因此，对 `get_user_pages` 的调用通常如下所示：
```c
down_read(&current->mm->mmap_sem);
result = get_user_pages(current, current->mm, ...);
up_read(&current->mm->mmap_sem);
```
返回值是实际映射的页面数，可能少于请求的页面数（但大于零）。

成功完成后，调用者会得到一个指向用户空间缓冲区的 `pages` 数组，该缓冲区已被锁定在内存中。为了直接操作该缓冲区，内核空间代码必须使用 `kmap` 或 `kmap_atomic` 将每个 `struct page` 指针转换为内核虚拟地址。不过，通常情况下，适合使用直接 I/O 的设备会使用 DMA 操作，因此你的驱动程序可能需要根据 `struct page` 指针数组创建一个散列/聚集列表。我们将在 15.4.4.7 节中讨论如何实现这一点。

一旦直接 I/O 操作完成，必须释放用户页面。但是，在释放之前，如果更改了这些页面的内容，必须通知内核。否则，内核可能会认为这些页面是“干净的”，即它们与交换设备上的副本一致，从而在不将其写回后备存储的情况下释放它们。因此，如果你更改了页面（响应于用户空间的读请求），必须通过调用以下函数将每个受影响的页面标记为脏页：
```c
void SetPageDirty(struct page *page);
```
（这个宏在 `<linux/page-flags.h>` 中定义）。大多数执行此操作的代码会先检查以确保页面不在内存映射的保留部分，因为保留部分的页面永远不会被换出。因此，代码通常如下所示：
```c
if (!PageReserved(page))
    SetPageDirty(page);
```
由于用户空间内存通常不会被标记为保留，严格来说这个检查并不是必需的，但当你深入到内存管理子系统时，最好还是做到全面和谨慎。

无论页面是否被更改，都必须将它们从页面缓存中释放，否则它们将永远留在那里。需要调用的函数是：
```c
void page_cache_release(struct page *page);
```
当然，如果需要，这个调用应该在页面被标记为脏页之后进行。

### 15.3.1 异步 I/O
2.6 内核新增的功能之一是异步 I/O 能力。异步 I/O 允许用户空间发起操作而无需等待操作完成；因此，应用程序可以在 I/O 操作进行的同时进行其他处理。一个复杂的高性能应用程序还可以使用异步 I/O 同时进行多个操作。

异步 I/O 的实现是可选的，很少有驱动程序开发者会去实现它；大多数设备无法从这种能力中受益。正如我们将在后续章节中看到的，块设备驱动程序和网络驱动程序始终是完全异步的，因此只有字符驱动程序才有可能需要显式支持异步 I/O。如果有充分的理由在任何给定时间有多个 I/O 操作未完成，字符设备可以从这种支持中受益。一个很好的例子是流式磁带驱动器，如果 I/O 操作到达的速度不够快，驱动器可能会停滞并显著减慢速度。一个试图从流式驱动器获得最佳性能的应用程序可以使用异步 I/O 在任何时候都有多个操作准备好执行。

对于极少数需要实现异步 I/O 的驱动程序开发者，我们简要概述一下其工作原理。我们在本章介绍异步 I/O，是因为其实现几乎总是也涉及直接 I/O 操作（如果你在内核中缓冲数据，通常可以在不增加用户空间复杂性的情况下实现异步行为）。

支持异步 I/O 的驱动程序应该包含 `<linux/aio.h>`。实现异步 I/O 有三个 `file_operations` 方法：
```c
ssize_t (*aio_read) (struct kiocb *iocb, char *buffer, 
                     size_t count, loff_t offset);
ssize_t (*aio_write) (struct kiocb *iocb, const char *buffer, 
                      size_t count, loff_t offset);
int (*aio_fsync) (struct kiocb *iocb, int datasync);
```
`aio_fsync` 操作主要与文件系统代码相关，因此我们在这里不再进一步讨论。另外两个方法 `aio_read` 和 `aio_write` 看起来与常规的 `read` 和 `write` 方法非常相似，但有一些不同之处。一是 `offset` 参数是按值传递的；异步操作永远不会改变文件位置，因此没有必要传递指向它的指针。这些方法还接受 `iocb`（“I/O 控制块”）参数，我们稍后会详细介绍。

`aio_read` 和 `aio_write` 方法的目的是发起一个读或写操作，这些操作在返回时可能完成也可能未完成。如果可以立即完成操作，方法应该完成操作并返回通常的状态：传输的字节数或负的错误码。因此，如果你的驱动程序有一个名为 `my_read` 的 `read` 方法，以下 `aio_read` 方法是完全正确的（尽管有点多余）：
```c
static ssize_t my_aio_read(struct kiocb *iocb, char *buffer, 
                           ssize_t count, loff_t offset)
{
    return my_read(iocb->ki_filp, buffer, count, &offset);
}
```
注意，`struct file` 指针可以在 `kiocb` 结构的 `ki_filp` 字段中找到。

如果你支持异步 I/O，必须意识到内核有时会创建“同步 IOCB”。本质上，这些是实际上必须同步执行的异步操作。人们可能会想为什么要这样做，但最好还是按照内核的要求去做。同步操作会在 IOCB 中标记；你的驱动程序应该使用以下函数查询该状态：
```c
int is_sync_kiocb(struct kiocb *iocb);
```
如果这个函数返回非零值，你的驱动程序必须同步执行该操作。

然而，最终的目的是实现异步操作。如果你的驱动程序能够发起操作（或者简单地将其排队，直到将来某个时间可以执行），它必须做两件事：记住关于该操作的所有必要信息，并向调用者返回 `-EIOCBQUEUED`。记住操作信息包括安排对用户空间缓冲区的访问；一旦返回，你将不会再有机会在调用进程的上下文中访问该缓冲区。一般来说，这意味着你可能需要设置一个直接的内核映射（使用 `get_user_pages`）或 DMA 映射。`-EIOCBQUEUED` 错误码表示操作尚未完成，其最终状态将在稍后通知。

当“稍后”到来时，你的驱动程序必须通知内核操作已完成。这可以通过调用 `aio_complete` 来实现：
```c
int aio_complete(struct kiocb *iocb, long res, long res2);
```
这里，`iocb` 是最初传递给你的同一个 IOCB，`res` 是操作的通常结果状态。`res2` 是第二个结果码，将返回给用户空间；大多数异步 I/O 实现将 `res2` 设置为 0。一旦调用了 `aio_complete`，你就不应该再触碰 IOCB 或用户缓冲区。

#### 15.3.1.1 异步 I/O 示例
示例代码中的面向页面的 `scullp` 驱动程序实现了异步 I/O。实现很简单，但足以展示异步操作应该如何组织。

`aio_read` 和 `aio_write` 方法实际上做的事情不多：
```c
static ssize_t scullp_aio_read(struct kiocb *iocb, char *buf, size_t count,
        loff_t pos)
{
    return scullp_defer_op(0, iocb, buf, count, pos);
}

static ssize_t scullp_aio_write(struct kiocb *iocb, const char *buf,
        size_t count, loff_t pos)
{
    return scullp_defer_op(1, iocb, (char *) buf, count, pos);
}
```
这些方法只是调用一个通用函数：
```c
struct async_work {
    struct kiocb *iocb;
    int result;
    struct work_struct work;
};

static int scullp_defer_op(int write, struct kiocb *iocb, char *buf,
        size_t count, loff_t pos)
{
    struct async_work *stuff;
    int result;

    /* 趁现在还能访问缓冲区，进行复制操作 */
    if (write)
        result = scullp_write(iocb->ki_filp, buf, count, &pos);
    else
        result = scullp_read(iocb->ki_filp, buf, count, &pos);

    /* 如果这是一个同步 IOCB，我们现在就返回状态 */
    if (is_sync_kiocb(iocb))
        return result;

    /* 否则，将完成操作推迟几毫秒 */
    stuff = kmalloc (sizeof (*stuff), GFP_KERNEL);
    if (stuff == NULL)
        return result; /* 没有内存，现在就完成操作 */
    stuff->iocb = iocb;
    stuff->result = result;
    INIT_WORK(&stuff->work, scullp_do_deferred_op, stuff);
    schedule_delayed_work(&stuff->work, HZ/100);
    return -EIOCBQUEUED;
}
```
一个更完整的实现会使用 `get_user_pages` 将用户缓冲区映射到内核空间。我们选择简单处理，一开始就复制数据。然后调用 `is_sync_kiocb` 来查看这个操作是否必须同步完成；如果是，就返回结果状态，操作结束。否则，我们将相关信息存储在一个小结构体中，通过工作队列安排“完成”操作，并返回 `-EIOCBQUEUED`。此时，控制权返回给用户空间。

稍后，工作队列会执行我们的完成函数：
```c
static void scullp_do_deferred_op(void *p)
{
    struct async_work *stuff = (struct async_work *) p;
    aio_complete(stuff->iocb, stuff->result, 0);
    kfree(stuff);
}
```
在这里，只需要使用我们保存的信息调用 `aio_complete` 即可。当然，一个真正的驱动程序的异步 I/O 实现会更复杂一些，但基本结构是类似的。 